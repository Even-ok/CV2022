> 1. The matrix, $M_{i}=\left[\begin{array}{cc}
>    R_{i} & \mathbf{t}_{i} \\
>    \mathbf{0}^{T} & 1
>    \end{array}\right] \in \mathbb{R}^{4 \times 4}$, can represent Euclidean transformations, where $R_{i} \in \mathbb{R}^{3 \times 3}$ is an orthonormal matrix, $\operatorname{det}\left(\mathbf{R}_{i}\right)=1$, and $\mathbf{t}_{i} \in \mathbb{R}^{3 \times 1}$ is a vector. Please prove that the set {$M_i$} forms a group. 

*Answer:* 

We consider two sets:  
$$
SE(3) = \set{\mathbf{M}|\mathbf{M}=\left[\begin{array}{c:c}
\mathbf{R} & \mathbf{t} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right], \mathbf{R} \in R^{3 \times 3}, \mathbf{t} \in R^{3}, \mathbf{R}^{T} \mathbf{R}=\mathbf{R} \mathbf{R}^{T}=\mathbf{I}, det(\mathbf{R})=1}\\
G = \set{ \mathbf{R}\in\mathbb{R}^{3\times 3}|\mathbf{RR}^T = I, det(\mathbf{R})=1}.
$$
Let $g_1=R_1\in G,g_2=R_2\in G$, $M_1 = \left[\begin{array}{c:c}
g_1 & \mathbf{t_1} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right] \in A$, $M_2 = \left[\begin{array}{c:c}
g_2 & \mathbf{t_2} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right] \in A$ .



**(1) Verify closure.**

According our assumption, we can get $g_1g_2=R_1R_2$, which is true under ordinary matrix multiplication. 
$$
A_1\times A_2= \left[\begin{array}{c:c}
g_1 & \mathbf{t_1} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right] \times \left[\begin{array}{c:c}
g_2 & \mathbf{t_2} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right] \\
=\left[\begin{array}{c:c}
g_1g_2 & g_1\mathbf{t_2}+\mathbf{t_1} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right]
$$
where the “$\times$” refers to the standard multiplication operation between matrices.
$$
\because g_1g_2=R_1R_2\\
\therefore (g_1g_2)(g_1g_2)^T = (R_1R_2)(R_1R_2)^T=R1R2R_2^TR_1^T=I\\
\because det(g_1g_2)=det(R_1R_2)=det(R_1)det(R_2)=1\\
\therefore g_1g_2 \in G,\space  g_1\mathbf{t_2}+\mathbf{t_1} \in R^{3}\\
\therefore A_1 \times A_2 \in SE(3)
$$
The set is closed under the multiplication operation. In other words, if A and B are any two matrices in SE(3), AB $\in$ SE(3).



**(2) Verify associativity**

The multiplication operation between matrices is associative. In other words, if A, B, and C are any three matrices $\in$ SE(3), then (AB) C = A (BC). 



**(3) Verify identity element**

For every element A $\in$ SE(3), there is an identity element given by the 4×4 identity matrix, 
$$
\mathbf{I} = \left( \begin{array}{l}

     1 & 0 & 0 & 0\\
     0 & 1 & 0 & 0\\
     0 & 0 & 1 & 0\\
     0 & 0 & 0 & 1
\end{array}
\right ) \in SE(3),
$$
such that $\mathbf{AI=A}$.



**(4) Verify inverse element for each group element.**

We assume that $M = \left[\begin{array}{c:c}
g & \mathbf{t} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right] \in SE(3)$ , we first need to prove the inverse element of $g_1$ belongs to G. Let $g = R \in G$
$$
\because (R^TR)^{-1} = R^{-1}(R^{-1})^T=I, \space det(R^{-1})=\frac{1}{det(R)}=1\\
\therefore R^{-1}\in G\\
\because g = R, gR^{-1}=RR^{-1}=I, R^{-1}g=R^{-1}R=I.
$$
Therefore, the inverse element of g is $g^{-1},g^{-1}\in G$.

For $M\in SE(3)$, we can easily find out its inverse element:
$$
A^{-1}= \left[\begin{array}{c:c}
R^{-1} & -R^{-1}\mathbf{t} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right]=\left[\begin{array}{c:c}
R^{T} & -R^{T}\mathbf{t} \\
\hdashline \mathbf{0}_{1 \times 3} & 1
\end{array}\right],
$$
and $M^{-1}\in SE(3)$.

Therefore, the set {$M_i$} forms a group. 

------



> 2. When deriving the Harris corner detector, we get the following matrix *M* composed of first-order partial derivatives in a local image patch *w*, 
>
> $$
> M=\left[\begin{array}{cc}
> \sum_{\left(x_{i}, y_{i}\right) \in w}\left(I_{x}\right)^{2} & \sum_{\left(x_{i}, y_{i}\right) \in w}\left(I_{x} I_{y}\right) \\
> \sum_{\left(x_{i}, y_{i}\right) \in w}\left(I_{x} I_{y}\right) & \sum_{\left(x_{i}, y_{i}\right) \in w}\left(I_{y}\right)^{2}
> \end{array}\right],
> $$
>
> a) Please prove that *M* is positive semi-definite.
>
> b) In practice, *M* is usually positive definite. If *M* is positive definite, prove that in the Cartesian coordinate system, $[x, y] M\left[\begin{array}{l}
> x \\
> y
> \end{array}\right]=1$, represents an ellipse.
>
> c) Suppose that *M* is positive definite and its two eigen-values are $\lambda_1$ and $\lambda_2$ and  $\lambda_1>\lambda_2>0$.  For the ellipse defined by $[x, y] M\left[\begin{array}{l}
> x \\
> y
> \end{array}\right]=1$, prove that the length of its semi-major axis is $\frac{1}{\sqrt{\lambda_2}}$ while the length of its semi-minor axis is $\frac{1}{\sqrt{\lambda_1}}$.

*Answer:* 

**a)**

 Let $x=[u\space v]^{T}$
$$
x^{T} M x=[u\space v]\left[\begin{array}{cc}
I_{x}^{2} & I_{x} I_{y} \\
I_{x} I_{y} & I_{y}^{2}
\end{array}\right]\left[\begin{array}{l}
u \\
v
\end{array}\right]=u^{2} I_{x}^{2}+2 u v I_{x} I_{y}+v^{2} I_{y}^{2}=\left(u I_{x}+v I_{y}\right)^{2} \geq 0.
$$
For any $x\in R^2$, $x^TMx\geq 0$. Therefore,  *M* is positive semi-definite and hence eigenvalues are always non-negative.



**b)** 

From a), we can see that if *M* is positive definite, then $x^{T} M x>0$. Any equation  of ellipse centered at the origin can be written as a quadratic form $k=a_{1} u^{2}+a_{2} u v+a_{3} v^{2}, k>0$. Therefore, $[x, y] M\left[\begin{array}{l}
x \\ y\end{array}\right]=x^{2} I_{x}^{2}+2 x y I_{x} I_{y}+y^{2} I_{y}^{2}=1$ can represent an ellipse, where $k=1,a_1=x^2,a_2=2xy,a_3=y^2$.



**c)** 

According to Principal Axis Theorem, we can we can always diagonalize symmetric matrices – that is we can find a matrix $P$ for a symmetric matrix $M$ such what $P^{\top} M P=D$ where $D $ is a diagonal matrix. The columns of $P$  that produce this result are found to be the eigenvectors of $M$.

Let $x=Py$, where $x$ is in standard coordinates and $y$ is in a different coordinate system where the columns of $P $ are the basis vectors of that coordinate system. Then we can rewite the ellipse equation: 
$$
K=x^{\top} M x \top=y^{\top} P^{\top} M P y=y^{\top} D y,
$$
where $D $ is a diagonal matrix. Now we can get:
$$
K=y^{\top} D y=\left[\begin{array}{ll}
y_{1} & y_{2}
\end{array}\right]\left[\begin{array}{cc}
\lambda_{1} & 0 \\
0 & \lambda_{2}
\end{array}\right]\left[\begin{array}{l}
y_{1} \\
y_{2}
\end{array}\right]=\lambda_{1} y_{1}^{2}+\lambda_{2} y_{2}^{2}.
$$
 We’ve just eliminated our cross term. We can use the standard formula for an ellipse to match coefficients. We find the following:
$$
\begin{gathered}
\frac{x^{2}}{r_{1}^{2}}+\frac{y^{2}}{r_{2}^{2}}=k=\lambda_{1} y_{1}^{2}+\lambda_{2} y_{2}^{2} \\
\therefore r_{1}=\lambda_{1}^{-\frac{1}{2}} \text { and } r_{2}=\lambda_{2}^{-\frac{1}{2}}
\end{gathered}
$$


------



> 3. In the lecture, we talked about the least square method to solve an over-determined linear system $A \mathbf{x}=b, A \in \mathbb{R}^{m \times n}, \mathbf{x} \in \mathbb{R}^{n \times 1}, m>n, \operatorname{rank}(A)=n$. The closed form solution is $\mathbf{x}=\left(A^{T} A\right)^{-1} A^{T} b$.  Try to prove that $A^TA$ is non-singular (or in other words, it is invertible).

*Answer:*

We can prove $col(A^TA)=col(A)$ to demonstrate $rank(A^TA)=rank(A)$, according to Rank Theorem, to prove that $A^TA$  is non-singular (invertible).

Let $\mathcal{A}_{1}=\operatorname{col}(A) \text { and } \mathcal{A}_{2}=\operatorname{col}\left(A^{t} A\right)$, then we have:
$$
\mathcal{A}_{2}=\left\{A^{t} A \boldsymbol{x}: \boldsymbol{x} \in \mathbb{R}^{n}\right\} \subseteq\left\{A^{t} \boldsymbol{y}: \boldsymbol{y} \in \mathbb{R}^{m}\right\}=\mathcal{A}_{1}.
$$
Thus $\mathcal{A}_{1}^{\perp} \subseteq \mathcal{A}_{2}^{\perp}$.  In fact, by the defifinition of orthogonal complement. Indeed:
$$
\boldsymbol{u} \in \mathcal{A}_{1}^{\perp} \quad \Rightarrow \quad \boldsymbol{u} \cdot \boldsymbol{v}=0, \forall \boldsymbol{v} \in \mathcal{A}_{1} \quad \Rightarrow \quad \boldsymbol{u} \cdot \boldsymbol{v}=0, \forall \boldsymbol{v} \in \mathcal{A}_{2} \quad \Rightarrow \quad \boldsymbol{u} \in \mathcal{A}_{2}^{\perp}.
$$
However, we can also show $\mathcal{A}_{2}^{\perp} \subseteq \mathcal{A}_{1}^{\perp}$ as follow:
$$
\boldsymbol{u} \in \mathcal{A}_{2}^{\perp} \quad \Rightarrow \quad \boldsymbol{u}^{t} A^{t} A=0 \quad \Rightarrow \quad\|A \boldsymbol{u}\|^{2}=\boldsymbol{u}^{t} A^{t} A \boldsymbol{u}=0 \quad \Rightarrow \quad \boldsymbol{u}^{t} A^{t}=0 \quad \Rightarrow \quad \boldsymbol{u} \in \mathcal{A}_{1}^{\perp}
$$
Thus $\mathcal{A}_{1}^{\perp}=\mathcal{A}_{2}^{\perp}$ and $\mathcal{A}_{1}=\left(\mathcal{A}_{1}^{\perp}\right)^{\perp}=\left(\mathcal{A}_{2}^{\perp}\right)^{\perp}=\mathcal{A}_{2}$, that is, $col(A^TA)=col(A)$.

Therefore, is $A\in \mathbb{R}^{m\times n}$ has full column rank (rank(A) = *n*) , then rank($A^TA$)=n. So $A^TA$ is non-singular.